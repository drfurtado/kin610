{
  "hash": "2cba9dd260b87f7420ea2b3f4616f8ad",
  "result": {
    "markdown": "---\ntitle: \"Week 12: One-way ANOVA\"\nsubtitle: |\n           KIN 610 - Spring 2023\nauthor: \"Dr. Ovande Furtado Jr\"\nfooter:  \"[https://drfurtado.github.io/kin610/](https://drfurtado.github.io/kin610/)\"\nlogo: \"images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    navigation-mode: linear\n    multiplex: false\n    incremental: false\n    transition: fade\n    slide-number: true\n    controls: true\n    controls-tutorial: true\n    controls-layout: edges\n    chalkboard: true\n    link-external-newwindow: true\n  pdf:\n   toc: true\neditor: visual\nexecute:\n  freeze: auto\nbibliography: ../references.bib\n---\n\n\n\n# Credits\n\n@furtadoRandomStatsOneWayANOVA2023\n\n# Sample data\n\n[RandomStats - One-Way ANOVA (drfurtado.github.io)](https://drfurtado.github.io/randomstats/posts/04082023-one-way-anova/#sec-sample-data)\n\n## Intro to One-way ANOVA\n\nOne-way Analysis of Variance (ANOVA) is a statistical technique used to compare the means of three or more groups. It is an extension of the t-test, which can only compare the means of two groups. The main purpose of one-way ANOVA is to determine if there are any significant differences among the group means.\n\n## Hypotheses in One-way ANOVA\n\n-   Null Hypothesis (H0): All group means are equal.\n-   Alternative Hypothesis (H1): At least one group mean is different.\n\n## One-way ANOVA Process\n\n1.  State the null and alternative hypotheses.\n2.  Calculate the test statistic (F-statistic) using the ANOVA table.\n3.  Determine the critical value based on the chosen significance level (alpha) and degrees of freedom.\n4.  Compare the test statistic to the critical value.\n5.  Make a decision about the null hypothesis.\n\n## Post-hoc Tests\n\nIf the null hypothesis is rejected, post-hoc tests can be conducted to identify the specific group differences.\n\n# Equations\n\n## Sum of Squares Between groups (SSB)\n\n-   Measures the variability between the group means\n-   Quantifies differences in the means of different groups compared to the grand mean\n-   Larger SSB value indicates greater difference between group means\n-   Suggests factor being studied may have significant impact on the outcome\n\n## Sum of Squares Within groups (SSW)\n\n-   Measures the variability within each group\n-   Represents dispersion of individual data points around their respective group means\n-   Larger SSW value indicates more variability within the groups\n-   May be due to random errors or other factors not accounted for in the study\n\n## Key Differences Between SSB and SSW\n\n-   SSB: concerned with variability between group means\n-   SSW: focuses on variability within each group\n-   SSB: identifies the effect of the factor being studied\n-   SSW: accounts for random errors and other unexplained factors\n\n## F-Ratio in One-Way ANOVA\n\n-   Test statistic for determining statistically significant differences between group means\n-   F-ratio = Mean Square Between groups (MSB) / Mean Square Within groups (MSW)\n-   MSB = SSB / degrees of freedom between groups\n-   MSW = SSW / degrees of freedom within groups\n-   Larger F-ratio indicates greater between-group variability than within-group variability\n-   Suggests differences between group means are statistically significant\n\n## Calculate the sum of squares: {.scrollable .smaller}\n\nSum of squares is calculated using the following components:\n\n-   **Sum of squares between groups (SSB): It measures the variability among group means.**\n\n    $$\n    SSB = Σk(Ŷi. - Ŷ..)² / ni\n    $$\n\n    where k is the number of groups, Ŷi. is the mean of group i, Ŷ.. is the grand mean, and ni is the number of observations in group i.\n\n-   **Sum of squares within groups (SSW): It measures the variability within each group.**\n\n    $$\n    SSW = ΣΣ(Yij - Ŷi.)²\n    $$\n\n    where Yij is the observation j in group i, and Ŷi. is the mean of group i.\n\n-   **Total sum of squares (SST): It measures the total variability in the data.**\n\n    $$\n    SST = ΣΣ(Yij - Ŷ..)²\n    $$\n\n## Calculate the F-ratio {.scrollable .smaller}\n\nThe F-ratio is calculated using the mean squares, which are obtained by dividing the sum of squares by their respective degrees of freedom.\n\n-   **Mean squares between groups (MSB):**\n\n    $$\n    MSB = SSB / (k - 1)\n    $$\n\n    where k is the number of groups.\n\n-   **Mean squares within groups (MSW):**\n\n    $$\n    MSW = SSW / (N - k)\n    $$\n\n    where N is the total number of observations.\n\n-   **F-ratio:**\n\n    $$\n    F = MSB / MSW\n    $$\n\nThe F-ratio follows an F-distribution with (k - 1) and (N - k) degrees of freedom. The F-ratio is then compared to the critical value from the F-distribution table at a given significance level (usually α = 0.05) to determine if the null hypothesis can be rejected.\n\n# F Distribution\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: pacman\n```\n:::\n\n::: {.cell-output-display}\n![](lec-10_files/figure-pdf/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n\n\n## Intro\n\nThe F distribution, also known as the Fisher-Snedecor distribution, is a continuous probability distribution that is widely used in statistical hypothesis testing, particularly in the analysis of variance (ANOVA). It is named after Ronald A. Fisher and George W. Snedecor, two prominent statisticians who contributed significantly to its development.\n\n## Characteristics of the F Distribution {.scrollable .smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-10_files/figure-pdf/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n::: columns\nThe F distribution has two important parameters: degrees of freedom for the numerator (df1) and degrees of freedom for the denominator (df2). These parameters define the shape of the distribution. Some key characteristics of the F distribution are:\n\n-   It is always non-negative, as it represents the ratio of two chi-square distributions.\n-   It is asymmetric and positively skewed, with a longer tail on the right side.\n-   The peak of the distribution shifts to the right as the degrees of freedom increase.\n-   As both degrees of freedom approach infinity, the F distribution converges to a normal distribution.\n:::\n:::\n:::\n\n## Applications of the F Distribution in ANOVA {.scrollable .smaller}\n\nThe F distribution is central to the analysis of variance (ANOVA) and other statistical tests that involve comparing variances or assessing the effects of different factors on a response variable. In these applications, an F statistic is calculated as the ratio of two mean square values (MS), which are derived from sums of squares (SS) and degrees of freedom:\n\nF = (MS_between groups) / (MS_within groups)\n\n-   The numerator (MS_between groups) represents the variability between the groups\n-   The denominator (MS_within groups) represents the variability within the groups\n-   F statistic is used in ANOVA (Analysis of Variance) to compare variances between groups\n-   It measures the ratio of variability between groups to the variability within groups\n\n***F Statistic and Group Differences***\n\n-   F statistic close to 1: No significant difference between groups\n-   F statistic much greater than 1: Significant effect of the factor being tested\n\n***Hypothesis Testing with F Statistic***\n\n-   Compare the calculated F statistic to a critical value from the F distribution\n-   Use appropriate degrees of freedom\n-   If F statistic \\> critical value, reject the null hypothesis\n-   A significant F statistic suggests a significant effect of the factor being tested\n\n# Measure of effect size\n\n-   Common measures of effect size for one-way ANOVA include eta-squared ($\\eta^2$), partial eta-squared ($\\eta_p^2$), and omega-squared ($\\omega^2$).\n\n## Eta-squared ($\\eta^2$)\n\n-   Proportion of variance in the dependent variable explained by the independent variable.\n-   Ranges from 0 to 1.\n-   Equation: $\\eta^2 = \\frac{SS_{between}}{SS_{total}}$\n-   Rule of thumb: 0.01 (small), 0.06 (medium), 0.14 (large) effect sizes.\n\n## Partial eta-squared ($\\eta_p^2$)\n\n-   Variation of eta-squared that takes into account the degrees of freedom associated with the residual error term.\n-   Can be less biased in certain situations.\n-   Equation: $\\eta_p^2 = \\frac{SS_{between}}{SS_{between} + SS_{error}}$\n-   Same rule of thumb as eta-squared.\n\n## Omega-squared ($\\omega^2$) {.scrollable .smaller}\n\n-   Estimates the true population effect size.\n-   Typically used when the sample size is small and/or the population effect size is unknown.\n-   Equation: $\\omega^2 = \\frac{SS_{between} - (df_{between} \\times MS_{error})}{SS_{total} + MS_{error}}$\n-   Slightly different rule of thumb: 0.01 (small), 0.06 (medium), 0.14 (large) effect sizes.\n\n**Note**: In these equations, $SS_{between}$ represents the sum of squares between groups, $SS_{total}$ represents the total sum of squares, $SS_{error}$ represents the sum of squares error (also known as the residual sum of squares), $df_{between}$ represents the degrees of freedom between groups, and $MS_{error}$ represents the mean square error (calculated as $SS_{error} / df_{error}$).\n\n# Post-hoc analysis {#sec-post-hoc-analysis}\n\n-   One-way ANOVA determines significant differences among means of 3 or more groups.\n-   Post-hoc tests identify pairs of group means that differ significantly from each other.\n-   This presentation will cover common post-hoc tests and factors to consider when selecting the appropriate test.\n\n## Common Post-hoc Tests {.scrollable .smaller}\n\nTukey's HSD Test\n\n-   Widely used for pairwise comparisons with equal variances.\n-   Controls experiment-wise Type I error rate.\n-   Most powerful test with equal sample sizes.\n\nBonferroni Test\n\n-   Adjusts significance level by the number of pairwise comparisons.\n-   More conservative than Tukey's HSD.\n-   Can be used with equal or unequal sample sizes.\n\nScheffé's Test\n\n-   More conservative than Tukey's HSD and Bonferroni.\n-   Allows any linear combination of means (not just pairwise comparisons).\n-   Suitable for complex planned comparisons.\n\nGames-Howell Test\n\n-   Designed for unequal variances.\n-   Uses Welch-Satterthwaite degrees of freedom.\n-   Suitable for unequal sample sizes.\n\n## Factors to Consider {.scrollable .smaller}\n\nAssumption of Equal Variances\n\n-   Tukey's HSD, Bonferroni, or Scheffé's if met.\n-   Games-Howell if not met.\n\nSample Sizes\n\n-   Tukey's HSD for equal sample sizes.\n-   Bonferroni or Games-Howell for unequal sample sizes.\n\nConservativeness vs. Power\n\n-   Tukey's HSD is more powerful but less conservative.\n-   Bonferroni and Scheffé's are more conservative but less powerful.\n-   Balance trade-off between Type I and Type II error rates.\n\n# Result interpretation {.scrollable .smaller}\n\n## ANOVA output includes:\n\n-   Degrees of freedom for the between-group factor and the residual error term\n-   Sum of squares for the between-group factor and the residual error term\n-   Mean squares for the between-group factor and the residual error term\n-   F-statistic and p-value associated with the F-statistic\n\n## To interpret the results, consider both the F-statistic and p-value:\n\n-   F-statistic indicates significant differences between group means\n-   P-value tells you whether those differences are statistically significant\n-   If p-value \\< alpha level, then evidence of significant difference between at least 2 group means\n\n## Examining effect size:\n\n-   Common measures: eta-squared ($\\eta^2$), partial eta-squared ($\\eta_p^2$), and omega-squared ($\\omega^2$)\n-   Larger effect sizes indicate more meaningful differences between groups\n\n# One-Way ANOVA Example {#sec-one-way-anova-example}\n\n## Research question\n\n-   Is there a significant difference in flexibility among participants in the three different exercise programs (A, B, and C)?\n\n## Hypothesis Statements\n\n-   Null Hypothesis (H0): $\\mu_{A} = \\mu_{B} = \\mu_{C}$\n    -   There is no significant difference in flexibility among the three exercise programs.\n-   Alternative Hypothesis (H1): $\\mu_{A} \\neq \\mu_{B} \\text{ or } \\mu_{A} \\neq \\mu_{C} \\text{ or } \\mu_{B} \\neq \\mu_{C}$\n    -   There is a significant difference in flexibility among at least two of the exercise programs.\n\n## ANOVA Test Results\n\n-   The one-way ANOVA test was conducted with a significance level of $\\alpha = 0.05$.\n-   The test statistic value was F(2, 27) = 4.98, and the p-value was 0.014.\n-   Since the p-value is less than the significance level, we reject the null hypothesis.\n-   We can conclude that there is a significant difference in flexibility among at least two of the exercise programs.\n\n## Post Hoc Tests {.scrollable .smaller}\n\n-   Since the one-way ANOVA test indicated a significant difference, we can conduct post hoc tests to determine which pairs of exercise programs have a significant difference in flexibility.\n-   Tukey's HSD test was conducted with a significance level of $\\alpha = 0.05$.\n-   The test results indicated a significant difference in flexibility between exercise programs A and C, with a p-value of 0.013.\n-   No other pairs of exercise programs had a significant difference in flexibility.\n\n# Statistical Package {.scrollable .smaller}\n\n## jamovi\n\n![](images/Screenshot%202023-04-08%20at%202.41.01%20PM.png)\n\n## jamovi - post hoc {.scrollable .smaller}\n\n![](images/Screenshot%202023-04-08%20at%202.42.26%20PM.png)\n\n## Interpreting the results {.scrollable .smaller}\n\n-   One-way ANOVA results\n    -   Statistically significant difference in flexibility scores between the three groups\n    -   F-value of 53.65 and p-value of 2.71e-12\n    -   At least one group mean is significantly different from the others\n-   Tukey HSD test results\n    -   Difference in flexibility scores between groups A and B is significantly different from 0\n    -   95% confidence interval: 5.97 to 10.96, p-value of 0\n    -   Group B has significantly higher flexibility score than group A\n-   No significant difference between groups A and C\n    -   Confidence interval (-3.83 to 1.16) includes 0, p-value of 0.40\n    -   Comparison between groups B and C shows significant difference\n    -   Group B has significantly higher flexibility score than group C\n    -   Confidence interval: -12.29 to -7.31, p-value of 0\n-   Conclusion\n    -   Significant differences in flexibility scores between at least two of the three groups\n    -   Group B has significantly higher flexibility score than both groups A and C\n    -   No significant difference in flexibility scores between groups A and C.\n\n## Results of One-Way ANOVA on Flexibility Scores {.scrollable .smaller}\n\n> A one-way analysis of variance (ANOVA) was conducted to determine whether there were significant differences in flexibility scores among three groups. Results revealed a significant main effect of group on flexibility scores, $F(2, 42) = 53.65, p < .001, \\eta_{p}^{2} = .72$. Post hoc pairwise comparisons using Tukey's HSD test showed that group B had significantly higher flexibility scores compared to both groups A ($p < .001$) and C ($p < .001$). However, no significant difference was found between groups A and C ($p = .40$). These findings suggest that group B had significantly better flexibility scores than groups A and C, while groups A and C did not differ significantly in terms of their flexibility scores.\n\n# Nonparametric\n\n## Intro\n\nThe nonparametric equivalent to the one-way ANOVA is the Kruskal-Wallis test. It is used when the assumptions of normality and homogeneity of variance are not met.\n\nThe Kruskal-Wallis test ranks the data and compares the medians of the groups instead of the means.\n\nLike the one-way ANOVA, it tests the null hypothesis that there is no difference between the groups, and the alternative hypothesis that at least one group differs from the others.\n\n## Jamovi\n\n1.  Open the data set in jamovi.\n\n2.  Click on the \"ANOVA\" button and under \"Nonparametric Tests\", select `One-way ANOVA Kruskal-Wallis`.\n\n3.  Drag the dependent variable to the \"Test Variable\" box and the grouping variable to the \"Factor\" box.\n\n4.  Click \"Run\" to obtain the test results.\n\n## SPSS\n\n1.  Open the data set in SPSS.\n\n2.  Click on \"Analyze\" and select \"Nonparametric Tests\" from the drop-down menu.\n\n3.  Select \"Independent Samples\" from the list of available tests.\n\n4.  Move the dependent variable to the \"Test Variable List\" box and the grouping variable to the \"Grouping Variable\" box.\n\n5.  Click on the \"Options\" button and select \"Kruskal-Wallis H\" from the list of available tests.\n\n6.  Click \"Continue\" and then \"OK\" to run the test.\n\n## R\n\n1.  Open R and load the necessary packages (e.g., \"tidyverse\", \"rstatix\").\n\n2.  Load the data set into R using the **`read.csv()`** or **`read.table()`** function.\n\n3.  Use the **`kruskal.test()`** function to conduct the Kruskal-Wallis test, specifying the dependent variable and grouping variable.\n\n4.  Use the **`summary()`** function to obtain the test results.\n\n5.  (Optional) Use the **`dunn_test()`** function from the \"rstatix\" package to conduct post-hoc pairwise comparisons.\n\n## References\n",
    "supporting": [
      "lec-10_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}