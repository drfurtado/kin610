{
  "hash": "42184ce15007a1ebb310bfbfa344ccb6",
  "result": {
    "markdown": "---\ntitle: \"Week 8: Regression Analysis \"\nsubtitle: |\n           KIN 610 - Spring 2023\nauthor: \"Dr. Ovande Furtado Jr\"\nfooter:  \"[https://drfurtado.github.io/kin610/](https://drfurtado.github.io/kin610/)\"\nlogo: \"images/logo.png\"\nformat: \n  revealjs: \n    theme: slides.scss\n    navigation-mode: linear\n    multiplex: false\n    incremental: false\n    transition: fade\n    slide-number: true\n    controls: true\n    controls-tutorial: true\n    controls-layout: edges\n    chalkboard: true\n    link-external-newwindow: true\neditor: visual\nexecute:\n  freeze: auto\nbibliography: references.bib\n---\n\n\n# Simple Linear Regression\n\n## Linear Regression Models\n\n-   A way of measuring the relationship between two variables\n\n-   Similar to Pearson correlation, but more powerful\n\n-   Can be used to predict one variable from another ( $y$ \\<\\-- $x$ )\n\n## Example: Parenthood Data Set {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n-   Bullet points:\n\n    -   Data set contains measures of sleep and grumpiness for Dani\n\n    -   Hypothesis: less sleep leads to more grumpiness\n\n    -   Scatterplot shows a strong negative correlation (r = -.90)\n:::\n\n::: {.column width=\"50%\"}\n![](images/image-1930564630.png)\n:::\n:::\n\n## Regression Line\n\n-   A straight line that best fits the data\n-   Represents the average relationship between the variables\n-   Can be used to estimate grumpiness from sleep\n\n## How to Draw a Regression Line?\n\n-   The line should go through the middle of the data\n\n-   The line should minimize the vertical distances between the data points and the line\n\n-   The line should have a slope and an intercept that can be calculated from the data\n\n## The formula for a straight line\n\n-   Usually written like this: $y = a + bx$\n\n-   Two variables: $x$ and $y$\n\n-   Two coefficients: $a$ and $b$\n\n-   Coefficient $a$ represents the `y-intercept` of the line\n\n-   Coefficient $b$ represents the `slope` of the line\n\n## The interpretation of intercept and slope\n\n-   Intercept: the value of $y$ that you get when $x$ = 0\n\n-   Slope: the change in $y$ that you get when you increase $x$ by 1 unit\n\n-   Positive slope: $y$ goes up as $x$ goes up\n\n-   Negative slope: $y$ goes down as $x$ goes up\n\n## The formula for a Regression line {.smaller}\n\n-   Same as the formula for a `straight line`, but with some `extra notation`\n\n-   So if $y$ is the outcome variable (DV) and $x$ is the predictor variable (IV), then:\n\n$$\\hat{y}_i = b_0 + b_1 x_i$$\n\n$\\hat{y}_i$: the predicted value of the `outcome variable` ($y$) for observation $i$\n\n${y}_i$: the actual value of the `outcome variable` ($y$) for observation $i$\n\n${x}_i$: the value of the `predictor variable` ($x$) for observation $i$\n\n${b}_0$: the estimated `intercept` of the regression line\n\n${b}_1$: the estimated `slope` of the regression line\n\n::: notes\nxi is the value of the predictor variable (#of hours on day 1) and yi is the corresponding value of the outcome variable (grumpiness on that day) - works for all observations.\n:::\n\n## The assumptions of the regression model {.smaller}\n\n-   We assume that the formula works for all observations in the data set (i.e., for all i)\n\n-   We distinguish between the actual data ${y}_i$ and the estimate $\\hat{y}_i$ (i.e., the prediction that our regression line is making)\n\n-   We use $b_0$ and $b_1$ to refer to the coefficients of the regression model\n\n    -   $b_0$: the estimated intercept of the regression line\n\n    -   $b_1$: the estimated slope of the regression line\n\n## Residuals of the Regression model {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec-8_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n\nNow, we have the complete linear regression model\n\n$$\\hat{y}_i = b_0 + b_1 x_i + {e}_i$$\n:::\n\n::: {.column width=\"40%\"}\n-   The data do not fall perfectly on the regression line\n\n-   The difference between the model prediction and that actual data point is called a residual, and we refer to it as ${e}_i$\n\n-   Mathematically, the residuals are defined as ${e}_i = {y}_i - \\hat{y}_i$\n\n-   The residuals measure how well the regression line fits the data\n\n    -   Smaller residuals: better fit\n    -   Larger residuals: worse fit\n:::\n:::\n\n## Estimating a linear regression model\n\n-   We want to find the regression line that fits the data best\n\n-   We can measure how well the regression line fits the data by looking at the residuals\n\n-   The residuals are the differences between the actual data and the model predictions\n\n-   Smaller residuals mean better fit, larger residuals mean worse fit\n\n## Ordinary least squares regression\n\n-   We use the method of `least squares` to estimate the `regression coefficients`\n\n-   The regression coefficients are estimates of the population parameters\n\n-   We use $\\hat{b}_0$ and $\\hat{b}_1$ to denote the estimated coefficients\n\n-   Ordinary least squares (OLS) regression is the most common way to estimate a linear regression model\n\n## How to find the estimated coefficients\n\n-   There are formulas to calculate $\\hat{b}_0$ and $\\hat{b}_1$ from the data\n\n-   The formulas involve some algebra and calculus that are not essential to understand the logic of regression\n\n-   We can use jamovi to do all the calculations for us\n\n-   jamovi will also provide other useful information about the regression model\n\n## Linear Regression in jamovi {.smaller}\n\n::: columns\n::: {.column width=\"30%\"}\n-   We can use jamovi to estimate a linear regression model from the data\n-   We need to specify the `dependent variable` and the `covariate(s)` in the analysis\n-   jamovi will output the estimated coefficients and other statistics\n:::\n\n::: {.column width=\"70%\"}\n![](images/image-1038690211.png)\n:::\n:::\n\n## Example: Parenthood data\n\nData file: parenthood.csv (found in module `lsj data` in jamovi)\n\nDependent variable: `dani.grump` (Dani's grumpiness)\n\nCovariate: `dani.sleep` (Dani's hours of sleep)\n\nEstimated intercept: $\\hat{b}_0$ = 125.96\n\nEstimated slope: $\\hat{b}_1$ = -8.94\n\nRegression equation: $\\hat{Y}_i = 125.96+(-8.94 X_i)$\n\n## Interpreting the estimated model\n\n-   We need to understand what the estimated coefficients mean\n-   The slope $\\hat{b}_1$ tells us how much the `dependent variable` changes when the `covariate` increases by one unit\n-   The intercept $\\hat{b}_0$ tells us what the expected value of the `dependent variable` is when the `covariate` is zero\n\n## Example: Parenthood data\n\n-   Dependent variable: `dani.grump` (Dani's grumpiness)\n-   Covariate: `dani.sleep` (Dani's hours of sleep)\n-   Estimated slope: $\\hat{b}_1$ = -8.94\n    -   Interpretation: Each additional hour of sleep `reduces` grumpiness by `8.94` points\n-   Estimated intercept: $\\hat{b}_0$ = 125.96\n    -   Interpretation: If Dani gets zero hours of sleep, her grumpiness will be `125.96` points\n\n# Multiple Regression\n\n## Introduction\n\n-   We can use more than one `predictor variable` to explain the variation in the `outcome variable`\n\n    -   Add more terms to our regression equation to represent each predictor variable\n\n-   Each term has a coefficient that indicates how much the outcome variable changes when that predictor variable increases by one unit\n\n## Example: Parenthood data {.smaller}\n\n-   Outcome variable: `dani.grump` (Dani's grumpiness)\n\n-   Predictor variables: `dani.sleep` (Dani's hours of sleep) **and** `baby.sleep` (Baby's hours of sleep)\n\nRegression equation: $Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i$\n\n$Y_i$: Dani's grumpiness on day $i$\n\n$X_{i1}$: Dani's hours of sleep on day $i$\n\n$X_{i2}$: Baby's hours of sleep on day $i$\n\n$b_0$: Intercept\n\n$b_1$: Coefficient for Dani's sleep\n\n$b_2$: Coefficient for Baby's sleep\n\n$\\epsilon_i$: Error term on day $i$\n\n## Estimating the coefficients in multiple regression\n\n-   We want to find the coefficients that minimize the sum of squared residuals\n-   Residuals are the differences between the observed and predicted values of the outcome variable\n-   We use a similar method as in `simple regression`, but with `more terms in the equation`\n\n## Doing it in jamovi {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/image-1898479417.png)\n:::\n\n::: {.column width=\"50%\"}\n-   jamovi can estimate multiple regression models easily\n-   We just need to add more variables to the `Covariates` box in the analysis\n-   jamovi will output the estimated coefficients and other statistics for each predictor variable\n-   The Table shows the coefficients for dani.sleep and baby.sleep as predictors of dani.grump\n:::\n:::\n\n## Interpreting the coefficients in multiple regression\n\n-   The coefficients tell us how much the `outcome variable` changes when `one predictor variable` increases by one unit, `holding` the other predictor variables `constant`\n-   The `larger` the **absolute** value of the coefficient, the `stronger` the effect of that predictor variable on the outcome variable\n-   The sign of the coefficient indicates whether the effect is positive or negative\n\n## Example: Parenthood data\n\n-   Coefficient (slope) for dani.sleep: `-8.94`\n\n    -   Interpretation: Each additional hour of sleep `reduces` Dani's grumpiness by `8.94 points`, regardless of how much sleep the baby gets\n\n-   Coefficient (slope) for baby.sleep: 0.01\n\n    -   Interpretation: Each additional hour of sleep for the baby `increases` Dani's grumpiness by `0.01 points`, regardless of how much sleep Dani gets\n\n## Quantifying the fit of the regression model\n\n-   We want to know how well our regression model predicts the outcome variable\n\n-   We can compare the predicted values ( $\\hat{Y}_i$ ) to the observed values ( $Y_i$ ) using two sums of squares\n\n    -   `Residual` sum of squares ( $SS_{res}$ ): measures how much error there is in our predictions\n\n    -   `Total` sum of squares ( $SS_{tot}$ ): measures how much variability there is in the outcome variable\n\n## The $R^2$ value (effect size) {.smaller}\n\n-   The $R^2$ value is a proportion that tells us how much of the `variability` in the `outcome variable` is explained by our `regression model`\n\n-   It is calculated as:\n\n$$R^2=1-\\frac{SS_{res}}{SS_{tot}}$$\n\n-   It ranges from 0 to 1, with `higher` values indicating `better fit`\n\n-   It can be interpreted as the `percentage of variance explained by our regression model`\n\n## The relationship between regression and correlation\n\n-   Regression and correlation are both ways of measuring the strength and direction of a linear relationship between two variables\n\n-   For a `simple regression` model with one predictor variable, the $R^2$ value is `equal` to the square of the Pearson correlation coefficient ($r^2$)\n\n    -   Running a Pearson correlation is equivalent to running a simple linear regression model\n\n## The adjusted $R^2$ value\n\n-   The adjusted $R^2$ value is a modified version of the $R^2$ value that takes into account the number of predictors in the model\n    -   The adjusted $R^2$ value adjusts for the degrees of freedom in the model\n-   It increases only `if adding a predictor` improves the model more than expected by chance\n\n## Which one to report: $R^2$ or adjusted $R^2$?\n\n-   There is no definitive answer to this question\n-   It depends on your preference and your research question\n-   Some factors to consider are:\n    -   Interpretability: $R^2$ is easier to understand and explain\n\n    -   Bias correction: Adjusted $R^2$ is less likely to overestimate the model performance\n\n    -   Hypothesis testing: There are other ways to test if adding a predictor improves the model significantly\n\n## Hypothesis tests for regression models {.smaller}\n\n-   We can use hypothesis tests to evaluate the `significance` of our regression model and its `coefficients`\n-   There are two types of hypothesis tests for regression models:\n    -   Testing the `model as a whole`: Is there any relationship between the predictors and the outcome?\n\n    -   Testing a `specific coefficient`: Is a particular predictor significantly related to the outcome?\n\n## Test the model as a whole {.smaller}\n\n$H_0$: there is no relationship between the predictors and the outcome\n\n$H_a$: data follow the regression model\n\n$$F=\\frac{(R^2/K)}{(1-R^2)/(N-K-1)}$$\n\n-   where $R^2$ is the proportion of variance explained by our model, $K$ is the number of predictors, and $N$ is the number of observations\n-   The F-test statistic follows an F-distribution with $K$ and $N-K-1$ degrees of freedom\n-   We can use a `p-value` to determine if our F-test statistic `is significant`\n-   {{< bi emoji-smile >}} jamovi can do this for us!\n\n## Tests for Individual Coefficients\n\n-   The F-test checks if the model as a whole is performing better than chance\n\n-   If the F-test is not significant, then the regression model may not be good\n\n-   However, passing the F-test does not imply that the model is good {{< bi emoji-frown >}}\n\n## Example of Multiple Linear Regression\n\n-   In a multiple linear regression model with baby.sleep and dani.sleep as predictors:\n\n    -   The estimated regression coefficient for baby.sleep is small (0.01) compared to dani.sleep (-.8.95)\n\n    -   This suggests that only dani.sleep matters in predicting grumpiness\n\n## Hypothesis Testing for Regression Coefficients\n\n-   A t-test can be used to test if a regression coefficient is significantly different from zero\n\n$H_0$: b = 0 (the true regression coefficient is zero)\n\n$H_0$: b ≠ 0 (the true regression coefficient is not zero)\n\n## Running Hypothesis Tests in Jamovi\n\n-   To compute statistics, check relevant options and run regression in jamovi\n\n-   See result in the next slide\n\n## Output\n\n![](images/image-65477197.png)\n\n::: notes\nModel Coefficients\n\n-   Located at bottom of jamovi analysis results\n-   Each row refers to one coefficient in regression model\n-   First row is intercept term; later rows look at each predictor\n\nCoefficient Information\n\n-   First column: estimate of b\n-   Second column: standard error estimate\n-   Third and fourth columns: lower and upper values for 95% confidence interval around b estimate\n-   Fifth column: t-statistic ( $t = b / se(b)$ )\n-   Last column: p-value for each test\n\nDegrees of Freedom\n\n-   Not listed in coefficients table itself\n-   Always N - K - 1\n-   Listed in table at top of output\n:::\n\n## Interpretation {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/image-1183448681.png)\n\nConclusion\n\n-   The current regression model may not be the best fit for the data\n-   Dropping `baby.sleep` predictor entirely may `improve` the model\n:::\n\n::: {.column width=\"50%\"}\n-   The model performs significantly better than chance\n    -   $F(2,97) = 215.24$, $p< .001$\n\n    -   $R^2 = .81$ value indicates that the regression model accounts for 81% of the variability in the outcome measure\n-   Individual Coefficients\n    -   `baby.sleep` variable has no significant effect\n\n    -   All work in this model is being done by the `dani.sleep` variable\n:::\n:::\n\n## Assumptions of Regression {.smaller}\n\nThe linear regression model relies on several assumptions.\n\n-   Linearity: The relationship between X and Y is assumed to be linear.\n\n-   Independence: Residuals are assumed to be independent of each other.\n\n-   Normality: The residuals are assumed to be normally distributed.\n\n-   Equality of Variance: The standard deviation of the residual is assumed to be the same for all values of Y-hat.\n\n## Assumptions of Regression, cont. {.smaller}\n\nAlso...\n\n-   Uncorrelated Predictors: In a multiple regression model, predictors should not be too strongly correlated with each other.\n\n    -   Strongly correlated predictors (collinearity) can cause problems when evaluating the model.\n\n-   No \\\"Bad\\\" Outliers: The regression model should not be too strongly influenced by one or two anomalous data points.\n\n    -   Anomalous data points can raise questions about the adequacy of the model and trustworthiness of data.\n\n## References\n",
    "supporting": [
      "lec-8_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}