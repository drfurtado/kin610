[
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n\nWe will meet at the Redwood Hall Computer Lab (Re 276) at 4 pm.\nDeadlines\n\nQuiz\nTakeaways"
  },
  {
    "objectID": "weeks/week-8.html#prepare",
    "href": "weeks/week-8.html#prepare",
    "title": "Week 8",
    "section": "Prepare",
    "text": "Prepare\nSimple & Multiple Linear Regression\n\nRead (Navarro & Foxcroft, 2022), chap. 12.3-12.12]\nWatch the video(s) below:\n\nRegression overview datalabcc (2018)\nLinear regression datalabcc (2019a)\nRegression diagnostics datalabcc (2019b)"
  },
  {
    "objectID": "weeks/week-8.html#participate",
    "href": "weeks/week-8.html#participate",
    "title": "Week 8",
    "section": "Participate",
    "text": "Participate\nThe slides can found here1"
  },
  {
    "objectID": "weeks/week-8.html#practice",
    "href": "weeks/week-8.html#practice",
    "title": "Week 8",
    "section": "Practice",
    "text": "Practice\nComing soon"
  },
  {
    "objectID": "weeks/week-8.html#perform",
    "href": "weeks/week-8.html#perform",
    "title": "Week 8",
    "section": "Perform",
    "text": "Perform\n\nTake the quiz\nComplete the major takeaways assignment\n\n\n\nBack to course schedule ⏎"
  },
  {
    "objectID": "weeks/week-8.html#footnotes",
    "href": "weeks/week-8.html#footnotes",
    "title": "Week 8",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe link will take you to a html page with the slides content. To view the actual slides (in presentation view) click on Revealjs under Other Formats.↩︎"
  },
  {
    "objectID": "slides/lec-8.qmd.html#linear-regression-models",
    "href": "slides/lec-8.qmd.html#linear-regression-models",
    "title": "Week 8: Regression Analysis",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\n\nA way of measuring the relationship between two variables\nSimilar to Pearson correlation, but more powerful\nCan be used to predict one variable from another"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-parenthood-data-set",
    "href": "slides/lec-8.qmd.html#example-parenthood-data-set",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood Data Set",
    "text": "Example: Parenthood Data Set\n\n\n\nData set contains measures of sleep and grumpiness for Dani\nHypothesis: less sleep leads to more grumpiness\nScatterplot shows a strong negative correlation (r = -.90)"
  },
  {
    "objectID": "slides/lec-8.qmd.html#regression-line",
    "href": "slides/lec-8.qmd.html#regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "Regression Line",
    "text": "Regression Line\n\nA straight line that best fits the data\nRepresents the average relationship between the variables\nCan be used to estimate grumpiness from sleep"
  },
  {
    "objectID": "slides/lec-8.qmd.html#how-to-draw-a-regression-line",
    "href": "slides/lec-8.qmd.html#how-to-draw-a-regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "How to Draw a Regression Line?",
    "text": "How to Draw a Regression Line?\n\nThe line should go through the middle of the data\nThe line should minimize the vertical distances between the data points and the line\nThe line should have a slope and an intercept that can be calculated from the data"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-formula-for-a-straight-line",
    "href": "slides/lec-8.qmd.html#the-formula-for-a-straight-line",
    "title": "Week 8: Regression Analysis",
    "section": "The formula for a straight line",
    "text": "The formula for a straight line\n\nUsually written like this: \\(y = a + bx\\)\nTwo variables: \\(x\\) and \\(y\\)\nTwo coefficients: \\(a\\) and \\(b\\)\nCoefficient \\(a\\) represents the y-intercept of the line\nCoefficient \\(b\\) represents the slope of the line"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-interpretation-of-intercept-and-slope",
    "href": "slides/lec-8.qmd.html#the-interpretation-of-intercept-and-slope",
    "title": "Week 8: Regression Analysis",
    "section": "The interpretation of intercept and slope",
    "text": "The interpretation of intercept and slope\n\nIntercept: the value of \\(y\\) that you get when \\(x\\) = 0\nSlope: the change in \\(y\\) that you get when you increase \\(x\\) by 1 unit\nPositive slope: \\(y\\) goes up as \\(x\\) goes up\nNegative slope: \\(y\\) goes down as \\(x\\) goes up"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-formula-for-a-regression-line",
    "href": "slides/lec-8.qmd.html#the-formula-for-a-regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "The formula for a Regression line",
    "text": "The formula for a Regression line\n\nSame as the formula for a straight line, but with some extra notation\nSo if \\(y\\) is the outcome variable (DV) and \\(x\\) is the predictor variable (IV), then:\n\n\\[\\hat{y}_i = b_0 + b_1 x_i\\]\n\\(\\hat{y}_i\\): the predicted value of the outcome variable (\\(y\\)) for observation \\(i\\)\n\\({y}_i\\): the actual value of the outcome variable (\\(y\\)) for observation \\(i\\)\n\\({x}_i\\): the value of the predictor variable (\\(x\\)) for observation \\(i\\)\n\\({b}_0\\): the estimated intercept of the regression line\n\\({b}_1\\): the estimated slope of the regression line\n\nxi is the value of the predictor variable (#of hours on day 1) and yi is the corresponding value of the outcome variable (grumpiness on that day) - works for all observations."
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-assumptions-of-the-regression-model",
    "href": "slides/lec-8.qmd.html#the-assumptions-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "The assumptions of the regression model",
    "text": "The assumptions of the regression model\n\nWe assume that the formula works for all observations in the data set (i.e., for all i)\nWe distinguish between the actual data \\({y}_i\\) and the estimate \\(\\hat{y}_i\\) (i.e., the prediction that our regression line is making)\nWe use \\(b_0\\) and \\(b_1\\) to refer to the coefficients of the regression model\n\n\\(b_0\\): the estimated intercept of the regression line\n\\(b_1\\): the estimated slope of the regression line"
  },
  {
    "objectID": "slides/lec-8.qmd.html#residuals-of-the-regression-model",
    "href": "slides/lec-8.qmd.html#residuals-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Residuals of the Regression model",
    "text": "Residuals of the Regression model\n\n\n\n# Generate some example data with a strong negative correlation\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- -0.8*x + rnorm(100, sd=0.5)\n\n# Plot the data\nplot(x,y)\n\n# Add the best fit line\nabline(lm(y ~ x), col=\"red\")\n\n\n\n\nNow, we have the complete linear regression model\n\\[\\hat{y}_i = b_0 + b_1 x_i + {e}_i\\]\n\n\nThe data do not fall perfectly on the regression line\nThe difference between the model prediction and that actual data point is called a residual, and we refer to it as \\({e}_i\\)\nMathematically, the residuals are defined as \\({e}_i = {y}_i - \\hat{y}_i\\)\nThe residuals measure how well the regression line fits the data\n\nSmaller residuals: better fit\nLarger residuals: worse fit"
  },
  {
    "objectID": "slides/lec-8.qmd.html#estimating-a-linear-regression-model",
    "href": "slides/lec-8.qmd.html#estimating-a-linear-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Estimating a linear regression model",
    "text": "Estimating a linear regression model\n\nWe want to find the regression line that fits the data best\nWe can measure how well the regression line fits the data by looking at the residuals\nThe residuals are the differences between the actual data and the model predictions\nSmaller residuals mean better fit, larger residuals mean worse fit"
  },
  {
    "objectID": "slides/lec-8.qmd.html#ordinary-least-squares-regression",
    "href": "slides/lec-8.qmd.html#ordinary-least-squares-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Ordinary least squares regression",
    "text": "Ordinary least squares regression\n\nWe use the method of least squares to estimate the regression coefficients\nThe regression coefficients are estimates of the population parameters\nWe use \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) to denote the estimated coefficients\nOrdinary least squares (OLS) regression is the most common way to estimate a linear regression model"
  },
  {
    "objectID": "slides/lec-8.qmd.html#how-to-find-the-estimated-coefficients",
    "href": "slides/lec-8.qmd.html#how-to-find-the-estimated-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "How to find the estimated coefficients",
    "text": "How to find the estimated coefficients\n\nThere are formulas to calculate \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) from the data\nThe formulas involve some algebra and calculus that are not essential to understand the logic of regression\nWe can use jamovi to do all the calculations for us\njamovi will also provide other useful information about the regression model"
  },
  {
    "objectID": "slides/lec-8.qmd.html#linear-regression-in-jamovi",
    "href": "slides/lec-8.qmd.html#linear-regression-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Linear Regression in jamovi",
    "text": "Linear Regression in jamovi\n\n\n\nWe can use jamovi to estimate a linear regression model from the data\nWe need to specify the dependent variable and the covariate(s) in the analysis\njamovi will output the estimated coefficients and other statistics"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-parenthood-data",
    "href": "slides/lec-8.qmd.html#example-parenthood-data",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\nData file: parenthood.csv (found in module lsj data in jamovi)\nDependent variable: dani.grump (Dani’s grumpiness)\nCovariate: dani.sleep (Dani’s hours of sleep)\nEstimated intercept: \\(\\hat{b}_0\\) = 125.96\nEstimated slope: \\(\\hat{b}_1\\) = -8.94\nRegression equation: \\(\\hat{Y}_i = 125.96+(-8.94 X_i)\\)"
  },
  {
    "objectID": "slides/lec-8.qmd.html#interpreting-the-estimated-model",
    "href": "slides/lec-8.qmd.html#interpreting-the-estimated-model",
    "title": "Week 8: Regression Analysis",
    "section": "Interpreting the estimated model",
    "text": "Interpreting the estimated model\n\nWe need to understand what the estimated coefficients mean\nThe slope \\(\\hat{b}_1\\) tells us how much the dependent variable changes when the covariate increases by one unit\nThe intercept \\(\\hat{b}_0\\) tells us what the expected value of the dependent variable is when the covariate is zero"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-parenthood-data-1",
    "href": "slides/lec-8.qmd.html#example-parenthood-data-1",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nDependent variable: dani.grump (Dani’s grumpiness)\nCovariate: dani.sleep (Dani’s hours of sleep)\nEstimated slope: \\(\\hat{b}_1\\) = -8.94\n\nInterpretation: Each additional hour of sleep reduces grumpiness by 8.94 points\n\nEstimated intercept: \\(\\hat{b}_0\\) = 125.96\n\nInterpretation: If Dani gets zero hours of sleep, her grumpiness will be 125.96 points"
  },
  {
    "objectID": "slides/lec-8.qmd.html#introduction",
    "href": "slides/lec-8.qmd.html#introduction",
    "title": "Week 8: Regression Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nWe can use more than one predictor variable to explain the variation in the outcome variable\n\nAdd more terms to our regression equation to represent each predictor variable\n\nEach term has a coefficient that indicates how much the outcome variable changes when that predictor variable increases by one unit"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-parenthood-data-2",
    "href": "slides/lec-8.qmd.html#example-parenthood-data-2",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nOutcome variable: dani.grump (Dani’s grumpiness)\nPredictor variables: dani.sleep (Dani’s hours of sleep) and baby.sleep (Baby’s hours of sleep)\n\nRegression equation: \\(Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\)\n\\(Y_i\\): Dani’s grumpiness on day \\(i\\)\n\\(X_{i1}\\): Dani’s hours of sleep on day \\(i\\)\n\\(X_{i2}\\): Baby’s hours of sleep on day \\(i\\)\n\\(b_0\\): Intercept\n\\(b_1\\): Coefficient for Dani’s sleep\n\\(b_2\\): Coefficient for Baby’s sleep\n\\(\\epsilon_i\\): Error term on day \\(i\\)"
  },
  {
    "objectID": "slides/lec-8.qmd.html#estimating-the-coefficients-in-multiple-regression",
    "href": "slides/lec-8.qmd.html#estimating-the-coefficients-in-multiple-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Estimating the coefficients in multiple regression",
    "text": "Estimating the coefficients in multiple regression\n\nWe want to find the coefficients that minimize the sum of squared residuals\nResiduals are the differences between the observed and predicted values of the outcome variable\nWe use a similar method as in simple regression, but with more terms in the equation"
  },
  {
    "objectID": "slides/lec-8.qmd.html#doing-it-in-jamovi",
    "href": "slides/lec-8.qmd.html#doing-it-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Doing it in jamovi",
    "text": "Doing it in jamovi\n\n\n\n\n\njamovi can estimate multiple regression models easily\nWe just need to add more variables to the Covariates box in the analysis\njamovi will output the estimated coefficients and other statistics for each predictor variable\nThe Table shows the coefficients for dani.sleep and baby.sleep as predictors of dani.grump"
  },
  {
    "objectID": "slides/lec-8.qmd.html#interpreting-the-coefficients-in-multiple-regression",
    "href": "slides/lec-8.qmd.html#interpreting-the-coefficients-in-multiple-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Interpreting the coefficients in multiple regression",
    "text": "Interpreting the coefficients in multiple regression\n\nThe coefficients tell us how much the outcome variable changes when one predictor variable increases by one unit, holding the other predictor variables constant\nThe larger the absolute value of the coefficient, the stronger the effect of that predictor variable on the outcome variable\nThe sign of the coefficient indicates whether the effect is positive or negative"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-parenthood-data-3",
    "href": "slides/lec-8.qmd.html#example-parenthood-data-3",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nCoefficient (slope) for dani.sleep: -8.94\n\nInterpretation: Each additional hour of sleep reduces Dani’s grumpiness by 8.94 points, regardless of how much sleep the baby gets\n\nCoefficient (slope) for baby.sleep: 0.01\n\nInterpretation: Each additional hour of sleep for the baby increases Dani’s grumpiness by 0.01 points, regardless of how much sleep Dani gets"
  },
  {
    "objectID": "slides/lec-8.qmd.html#quantifying-the-fit-of-the-regression-model",
    "href": "slides/lec-8.qmd.html#quantifying-the-fit-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Quantifying the fit of the regression model",
    "text": "Quantifying the fit of the regression model\n\nWe want to know how well our regression model predicts the outcome variable\nWe can compare the predicted values ( \\(\\hat{Y}_i\\) ) to the observed values ( \\(Y_i\\) ) using two sums of squares\n\nResidual sum of squares ( \\(SS_{res}\\) ): measures how much error there is in our predictions\nTotal sum of squares ( \\(SS_{tot}\\) ): measures how much variability there is in the outcome variable"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-r2-value-effect-size",
    "href": "slides/lec-8.qmd.html#the-r2-value-effect-size",
    "title": "Week 8: Regression Analysis",
    "section": "The \\(R^2\\) value (effect size)",
    "text": "The \\(R^2\\) value (effect size)\n\nThe \\(R^2\\) value is a proportion that tells us how much of the variability in the outcome variable is explained by our regression model\nIt is calculated as:\n\n\\[R^2=1-\\frac{SS_{res}}{SS_{tot}}\\]\n\nIt ranges from 0 to 1, with higher values indicating better fit\nIt can be interpreted as the percentage of variance explained by our regression model"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-relationship-between-regression-and-correlation",
    "href": "slides/lec-8.qmd.html#the-relationship-between-regression-and-correlation",
    "title": "Week 8: Regression Analysis",
    "section": "The relationship between regression and correlation",
    "text": "The relationship between regression and correlation\n\nRegression and correlation are both ways of measuring the strength and direction of a linear relationship between two variables\nFor a simple regression model with one predictor variable, the \\(R^2\\) value is equal to the square of the Pearson correlation coefficient (\\(r^2\\))\n\nRunning a Pearson correlation is equivalent to running a simple linear regression model"
  },
  {
    "objectID": "slides/lec-8.qmd.html#the-adjusted-r2-value",
    "href": "slides/lec-8.qmd.html#the-adjusted-r2-value",
    "title": "Week 8: Regression Analysis",
    "section": "The adjusted \\(R^2\\) value",
    "text": "The adjusted \\(R^2\\) value\n\nThe adjusted \\(R^2\\) value is a modified version of the \\(R^2\\) value that takes into account the number of predictors in the model\n\nThe adjusted \\(R^2\\) value adjusts for the degrees of freedom in the model\n\nIt increases only if adding a predictor improves the model more than expected by chance"
  },
  {
    "objectID": "slides/lec-8.qmd.html#which-one-to-report-r2-or-adjusted-r2",
    "href": "slides/lec-8.qmd.html#which-one-to-report-r2-or-adjusted-r2",
    "title": "Week 8: Regression Analysis",
    "section": "Which one to report: \\(R^2\\) or adjusted \\(R^2\\)?",
    "text": "Which one to report: \\(R^2\\) or adjusted \\(R^2\\)?\n\nThere is no definitive answer to this question\nIt depends on your preference and your research question\nSome factors to consider are:\n\nInterpretability: \\(R^2\\) is easier to understand and explain\nBias correction: Adjusted \\(R^2\\) is less likely to overestimate the model performance\nHypothesis testing: There are other ways to test if adding a predictor improves the model significantly"
  },
  {
    "objectID": "slides/lec-8.qmd.html#hypothesis-tests-for-regression-models",
    "href": "slides/lec-8.qmd.html#hypothesis-tests-for-regression-models",
    "title": "Week 8: Regression Analysis",
    "section": "Hypothesis tests for regression models",
    "text": "Hypothesis tests for regression models\n\nWe can use hypothesis tests to evaluate the significance of our regression model and its coefficients\nThere are two types of hypothesis tests for regression models:\n\nTesting the model as a whole: Is there any relationship between the predictors and the outcome?\nTesting a specific coefficient: Is a particular predictor significantly related to the outcome?"
  },
  {
    "objectID": "slides/lec-8.qmd.html#test-the-model-as-a-whole",
    "href": "slides/lec-8.qmd.html#test-the-model-as-a-whole",
    "title": "Week 8: Regression Analysis",
    "section": "Test the model as a whole",
    "text": "Test the model as a whole\n\\(H_0\\): there is no relationship between the predictors and the outcome\n\\(H_a\\): data follow the regression model\n\\[F=\\frac{(R^2/K)}{(1-R^2)/(N-K-1)}\\]\n\nwhere \\(R^2\\) is the proportion of variance explained by our model, \\(K\\) is the number of predictors, and \\(N\\) is the number of observations\nThe F-test statistic follows an F-distribution with \\(K\\) and \\(N-K-1\\) degrees of freedom\nWe can use a p-value to determine if our F-test statistic is significant\n jamovi can do this for us!"
  },
  {
    "objectID": "slides/lec-8.qmd.html#tests-for-individual-coefficients",
    "href": "slides/lec-8.qmd.html#tests-for-individual-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "Tests for Individual Coefficients",
    "text": "Tests for Individual Coefficients\n\nThe F-test checks if the model as a whole is performing better than chance\nIf the F-test is not significant, then the regression model may not be good\nHowever, passing the F-test does not imply that the model is good"
  },
  {
    "objectID": "slides/lec-8.qmd.html#example-of-multiple-linear-regression",
    "href": "slides/lec-8.qmd.html#example-of-multiple-linear-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Example of Multiple Linear Regression",
    "text": "Example of Multiple Linear Regression\n\nIn a multiple linear regression model with baby.sleep and dani.sleep as predictors:\n\nThe estimated regression coefficient for baby.sleep is small (0.01) compared to dani.sleep (-.8.95)\nThis suggests that only dani.sleep matters in predicting grumpiness"
  },
  {
    "objectID": "slides/lec-8.qmd.html#hypothesis-testing-for-regression-coefficients",
    "href": "slides/lec-8.qmd.html#hypothesis-testing-for-regression-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "Hypothesis Testing for Regression Coefficients",
    "text": "Hypothesis Testing for Regression Coefficients\n\nA t-test can be used to test if a regression coefficient is significantly different from zero\n\n\\(H_0\\): b = 0 (the true regression coefficient is zero)\n\\(H_0\\): b ≠ 0 (the true regression coefficient is not zero)"
  },
  {
    "objectID": "slides/lec-8.qmd.html#running-hypothesis-tests-in-jamovi",
    "href": "slides/lec-8.qmd.html#running-hypothesis-tests-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Running Hypothesis Tests in Jamovi",
    "text": "Running Hypothesis Tests in Jamovi\n\nTo compute statistics, check relevant options and run regression in jamovi\nSee result in the next slide"
  },
  {
    "objectID": "slides/lec-8.qmd.html#output",
    "href": "slides/lec-8.qmd.html#output",
    "title": "Week 8: Regression Analysis",
    "section": "Output",
    "text": "Output\n\n\nModel Coefficients\n\nLocated at bottom of jamovi analysis results\nEach row refers to one coefficient in regression model\nFirst row is intercept term; later rows look at each predictor\n\nCoefficient Information\n\nFirst column: estimate of b\nSecond column: standard error estimate\nThird and fourth columns: lower and upper values for 95% confidence interval around b estimate\nFifth column: t-statistic ( \\(t = b / se(b)\\) )\nLast column: p-value for each test\n\nDegrees of Freedom\n\nNot listed in coefficients table itself\nAlways N - K - 1\nListed in table at top of output"
  },
  {
    "objectID": "slides/lec-8.qmd.html#interpretation",
    "href": "slides/lec-8.qmd.html#interpretation",
    "title": "Week 8: Regression Analysis",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nConclusion\n\nThe current regression model may not be the best fit for the data\nDropping baby.sleep predictor entirely may improve the model\n\n\n\nThe model performs significantly better than chance\n\n\\(F(2,97) = 215.24\\), \\(p&lt; .001\\)\n\\(R^2 = .81\\) value indicates that the regression model accounts for 81% of the variability in the outcome measure\n\nIndividual Coefficients\n\nbaby.sleep variable has no significant effect\nAll work in this model is being done by the dani.sleep variable"
  },
  {
    "objectID": "slides/lec-8.qmd.html#assumptions-of-regression",
    "href": "slides/lec-8.qmd.html#assumptions-of-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Assumptions of Regression",
    "text": "Assumptions of Regression\nThe linear regression model relies on several assumptions.\n\nLinearity: The relationship between X and Y is assumed to be linear.\nIndependence: Residuals are assumed to be independent of each other.\nNormality: The residuals are assumed to be normally distributed.\nEquality of Variance: The standard deviation of the residual is assumed to be the same for all values of Y-hat."
  },
  {
    "objectID": "slides/lec-8.qmd.html#assumptions-of-regression-cont.",
    "href": "slides/lec-8.qmd.html#assumptions-of-regression-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Assumptions of Regression, cont.",
    "text": "Assumptions of Regression, cont.\nAlso…\n\nUncorrelated Predictors: In a multiple regression model, predictors should not be too strongly correlated with each other.\n\nStrongly correlated predictors (collinearity) can cause problems when evaluating the model.\n\nNo “Bad” Outliers: The regression model should not be too strongly influenced by one or two anomalous data points.\n\nAnomalous data points can raise questions about the adequacy of the model and trustworthiness of data."
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-linearity",
    "href": "slides/lec-8.qmd.html#checking-for-linearity",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for linearity",
    "text": "Checking for linearity\n\n\n\n\nChecking Linearity\n\nIt is important to check for the linearity of relationships between predictors and outcomes.\n\nPlotting Relationships\n\nOne way to check for linearity is to plot the relationship between predicted values and observed values for the outcome variable.\n\nUsing Jamovi\n\nIn Jamovi, you can save predicted values to the dataset and then draw a scatterplot of observed against predicted (fitted) values.\n\nInterpreting Results\n\nIf the plot looks approximately linear, then it suggests that your model is not doing too badly. However, if there are big departures from linearity, it suggests that changes need to be made."
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-linearity-cont.",
    "href": "slides/lec-8.qmd.html#checking-for-linearity-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for linearity, cont.",
    "text": "Checking for linearity, cont.\n\n\n\n\nTo get a more detailed picture of linearity, it can be helpful to look at the relationship between predicted values and residuals.\nUsing Jamovi\n\nIn Jamovi, you can save residuals to the dataset and then draw a scatterplot of predicted values against residual values.\n\nInterpreting Results\n\nIdeally, the relationship between predicted values and residuals should be a straight, perfectly horizontal line. In practice, we’re looking for a reasonably straight or flat line. This is a matter of judgement."
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-normality-residuals",
    "href": "slides/lec-8.qmd.html#checking-for-normality-residuals",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for normality (residuals)",
    "text": "Checking for normality (residuals)\n\n\n\n\nRegression models rely on a normality assumption: the residuals should be normally distributed.\nUsing Jamovi\n\nIn Jamovi, you can draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option.\n\nInterpreting Results\n\nThe output shows the standardized residuals plotted as a function of their theoretical quantiles according to the regression model. The dots should be somewhat near the line."
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-normality-residuals-cont.",
    "href": "slides/lec-8.qmd.html#checking-for-normality-residuals-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for normality (residuals), cont.",
    "text": "Checking for normality (residuals), cont.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChecking Relationship between Predicted Values and Residuals\n\nIn Jamovi, you can use the ‘Residuals Plots’ option to check the relationship between predicted values and residuals.\nThe output provides a scatterplot for each predictor variable, the outcome variable, and the predicted values against residuals.\n\nInterpreting Results\n\nWe are looking for a fairly uniform distribution of dots with no clear bunching or patterning.\n\nThe dots are fairly evenly spread across the whole plot \n\nIssues with the relationship between predicted values and residuals? \n\nTransform one or more of the variables (Box-Cox Transform in jamovi)"
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-equality-of-variance",
    "href": "slides/lec-8.qmd.html#checking-for-equality-of-variance",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for equality of variance",
    "text": "Checking for equality of variance\n\n\n\n\nRegression models make an assumption of equality (homogeneity) of variance.\n\nThis means that the variance of the residuals is assumed to be constant.\n\nPlotting Equality of Variance in Jamovi\n\nTo check this assumption in Jamovi, first calculate the square root of the absolute size of the residual.\n\nCompute this new variable using the formula SQRT(ABS(Residuals))\n\nThen plot this against the predicted values.\nThe plot should show a straight horizontal line running through the middle."
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-collineary",
    "href": "slides/lec-8.qmd.html#checking-for-collineary",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for Collineary",
    "text": "Checking for Collineary\n\n\n\n\n\nVariance Inflation Factors (VIFs) can be used to determine if predictors in a regression model are too highly correlated with each other.\n\nEach predictor has an associated VIF.\n\nIn Jamovi, click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options to see VIF values.\nInterpreting VIF\n\nA VIF of 1 means no correlation among the predictor and the remaining predictor variables\nVIFs exceeding 4 warrant further investigation\nVIFs exceeding 10 are signs of serious multicollinearity requiring correction"
  },
  {
    "objectID": "slides/lec-8.qmd.html#checking-for-outliers",
    "href": "slides/lec-8.qmd.html#checking-for-outliers",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for outliers",
    "text": "Checking for outliers\n\n\n\n\n\nUsed in regression analysis to identify influential data points that may negatively affect your regression model\nDatasets with a large number of highly influential points might not be suitable for linear regression without further processing such as outlier removal or imputation\nIdentifying Outliers\n\nA general rule of thumb: Cook’s distance greater than 1 is often considered large\n\nWhat if the value is greater than 1?\n\nremove the outlier and run the regression again\nHow? In jamovi you can save the Cook’s distance values to the dataset, then draw a boxplot of the Cook’s distance values to identify the specific outliers."
  },
  {
    "objectID": "slides/lec-8.qmd.html#references",
    "href": "slides/lec-8.qmd.html#references",
    "title": "Week 8: Regression Analysis",
    "section": "References",
    "text": "References\n\n\n\nhttps://drfurtado.github.io/kin610/\n\n\n\nNavarro, Danielle J, and David R Foxcroft. 2022. Learning Statistics with Jamovi: A Tutorial for Psychology Students and Other Beginners (Version 0.75). Danielle J. Navarro; David R. Foxcroft. https://doi.org/10.24384/HGC3-7P15."
  },
  {
    "objectID": "slides/lec-8.html",
    "href": "slides/lec-8.html",
    "title": "Week 8: Regression Analysis",
    "section": "",
    "text": "Navarro and Foxcroft (2022)"
  },
  {
    "objectID": "slides/lec-8.html#linear-regression-models",
    "href": "slides/lec-8.html#linear-regression-models",
    "title": "Week 8: Regression Analysis",
    "section": "Linear Regression Models",
    "text": "Linear Regression Models\n\nA way of measuring the relationship between two variables\nSimilar to Pearson correlation, but more powerful\nCan be used to predict one variable from another"
  },
  {
    "objectID": "slides/lec-8.html#example-parenthood-data-set",
    "href": "slides/lec-8.html#example-parenthood-data-set",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood Data Set",
    "text": "Example: Parenthood Data Set\n\n\n\nData set contains measures of sleep and grumpiness for Dani\nHypothesis: less sleep leads to more grumpiness\nScatterplot shows a strong negative correlation (r = -.90)"
  },
  {
    "objectID": "slides/lec-8.html#regression-line",
    "href": "slides/lec-8.html#regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "Regression Line",
    "text": "Regression Line\n\nA straight line that best fits the data\nRepresents the average relationship between the variables\nCan be used to estimate grumpiness from sleep"
  },
  {
    "objectID": "slides/lec-8.html#how-to-draw-a-regression-line",
    "href": "slides/lec-8.html#how-to-draw-a-regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "How to Draw a Regression Line?",
    "text": "How to Draw a Regression Line?\n\nThe line should go through the middle of the data\nThe line should minimize the vertical distances between the data points and the line\nThe line should have a slope and an intercept that can be calculated from the data"
  },
  {
    "objectID": "slides/lec-8.html#the-formula-for-a-straight-line",
    "href": "slides/lec-8.html#the-formula-for-a-straight-line",
    "title": "Week 8: Regression Analysis",
    "section": "The formula for a straight line",
    "text": "The formula for a straight line\n\nUsually written like this: \\(y = a + bx\\)\nTwo variables: \\(x\\) and \\(y\\)\nTwo coefficients: \\(a\\) and \\(b\\)\nCoefficient \\(a\\) represents the y-intercept of the line\nCoefficient \\(b\\) represents the slope of the line"
  },
  {
    "objectID": "slides/lec-8.html#the-interpretation-of-intercept-and-slope",
    "href": "slides/lec-8.html#the-interpretation-of-intercept-and-slope",
    "title": "Week 8: Regression Analysis",
    "section": "The interpretation of intercept and slope",
    "text": "The interpretation of intercept and slope\n\nIntercept: the value of \\(y\\) that you get when \\(x\\) = 0\nSlope: the change in \\(y\\) that you get when you increase \\(x\\) by 1 unit\nPositive slope: \\(y\\) goes up as \\(x\\) goes up\nNegative slope: \\(y\\) goes down as \\(x\\) goes up"
  },
  {
    "objectID": "slides/lec-8.html#the-formula-for-a-regression-line",
    "href": "slides/lec-8.html#the-formula-for-a-regression-line",
    "title": "Week 8: Regression Analysis",
    "section": "The formula for a Regression line",
    "text": "The formula for a Regression line\n\nSame as the formula for a straight line, but with some extra notation\nSo if \\(y\\) is the outcome variable (DV) and \\(x\\) is the predictor variable (IV), then:\n\n\\[\\hat{y}_i = b_0 + b_1 x_i\\]\n\\(\\hat{y}_i\\): the predicted value of the outcome variable (\\(y\\)) for observation \\(i\\)\n\\({y}_i\\): the actual value of the outcome variable (\\(y\\)) for observation \\(i\\)\n\\({x}_i\\): the value of the predictor variable (\\(x\\)) for observation \\(i\\)\n\\({b}_0\\): the estimated intercept of the regression line\n\\({b}_1\\): the estimated slope of the regression line\n\nxi is the value of the predictor variable (#of hours on day 1) and yi is the corresponding value of the outcome variable (grumpiness on that day) - works for all observations."
  },
  {
    "objectID": "slides/lec-8.html#the-assumptions-of-the-regression-model",
    "href": "slides/lec-8.html#the-assumptions-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "The assumptions of the regression model",
    "text": "The assumptions of the regression model\n\nWe assume that the formula works for all observations in the data set (i.e., for all i)\nWe distinguish between the actual data \\({y}_i\\) and the estimate \\(\\hat{y}_i\\) (i.e., the prediction that our regression line is making)\nWe use \\(b_0\\) and \\(b_1\\) to refer to the coefficients of the regression model\n\n\\(b_0\\): the estimated intercept of the regression line\n\\(b_1\\): the estimated slope of the regression line"
  },
  {
    "objectID": "slides/lec-8.html#residuals-of-the-regression-model",
    "href": "slides/lec-8.html#residuals-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Residuals of the Regression model",
    "text": "Residuals of the Regression model\n\n\n\n# Generate some example data with a strong negative correlation\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- -0.8*x + rnorm(100, sd=0.5)\n\n# Plot the data\nplot(x,y)\n\n# Add the best fit line\nabline(lm(y ~ x), col=\"red\")\n\n\n\n\nNow, we have the complete linear regression model\n\\[\\hat{y}_i = b_0 + b_1 x_i + {e}_i\\]\n\n\nThe data do not fall perfectly on the regression line\nThe difference between the model prediction and that actual data point is called a residual, and we refer to it as \\({e}_i\\)\nMathematically, the residuals are defined as \\({e}_i = {y}_i - \\hat{y}_i\\)\nThe residuals measure how well the regression line fits the data\n\nSmaller residuals: better fit\nLarger residuals: worse fit"
  },
  {
    "objectID": "slides/lec-8.html#estimating-a-linear-regression-model",
    "href": "slides/lec-8.html#estimating-a-linear-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Estimating a linear regression model",
    "text": "Estimating a linear regression model\n\nWe want to find the regression line that fits the data best\nWe can measure how well the regression line fits the data by looking at the residuals\nThe residuals are the differences between the actual data and the model predictions\nSmaller residuals mean better fit, larger residuals mean worse fit"
  },
  {
    "objectID": "slides/lec-8.html#ordinary-least-squares-regression",
    "href": "slides/lec-8.html#ordinary-least-squares-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Ordinary least squares regression",
    "text": "Ordinary least squares regression\n\nWe use the method of least squares to estimate the regression coefficients\nThe regression coefficients are estimates of the population parameters\nWe use \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) to denote the estimated coefficients\nOrdinary least squares (OLS) regression is the most common way to estimate a linear regression model"
  },
  {
    "objectID": "slides/lec-8.html#how-to-find-the-estimated-coefficients",
    "href": "slides/lec-8.html#how-to-find-the-estimated-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "How to find the estimated coefficients",
    "text": "How to find the estimated coefficients\n\nThere are formulas to calculate \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) from the data\nThe formulas involve some algebra and calculus that are not essential to understand the logic of regression\nWe can use jamovi to do all the calculations for us\njamovi will also provide other useful information about the regression model"
  },
  {
    "objectID": "slides/lec-8.html#linear-regression-in-jamovi",
    "href": "slides/lec-8.html#linear-regression-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Linear Regression in jamovi",
    "text": "Linear Regression in jamovi\n\n\n\nWe can use jamovi to estimate a linear regression model from the data\nWe need to specify the dependent variable and the covariate(s) in the analysis\njamovi will output the estimated coefficients and other statistics"
  },
  {
    "objectID": "slides/lec-8.html#example-parenthood-data",
    "href": "slides/lec-8.html#example-parenthood-data",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\nData file: parenthood.csv (found in module lsj data in jamovi)\nDependent variable: dani.grump (Dani’s grumpiness)\nCovariate: dani.sleep (Dani’s hours of sleep)\nEstimated intercept: \\(\\hat{b}_0\\) = 125.96\nEstimated slope: \\(\\hat{b}_1\\) = -8.94\nRegression equation: \\(\\hat{Y}_i = 125.96+(-8.94 X_i)\\)"
  },
  {
    "objectID": "slides/lec-8.html#interpreting-the-estimated-model",
    "href": "slides/lec-8.html#interpreting-the-estimated-model",
    "title": "Week 8: Regression Analysis",
    "section": "Interpreting the estimated model",
    "text": "Interpreting the estimated model\n\nWe need to understand what the estimated coefficients mean\nThe slope \\(\\hat{b}_1\\) tells us how much the dependent variable changes when the covariate increases by one unit\nThe intercept \\(\\hat{b}_0\\) tells us what the expected value of the dependent variable is when the covariate is zero"
  },
  {
    "objectID": "slides/lec-8.html#example-parenthood-data-1",
    "href": "slides/lec-8.html#example-parenthood-data-1",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nDependent variable: dani.grump (Dani’s grumpiness)\nCovariate: dani.sleep (Dani’s hours of sleep)\nEstimated slope: \\(\\hat{b}_1\\) = -8.94\n\nInterpretation: Each additional hour of sleep reduces grumpiness by 8.94 points\n\nEstimated intercept: \\(\\hat{b}_0\\) = 125.96\n\nInterpretation: If Dani gets zero hours of sleep, her grumpiness will be 125.96 points"
  },
  {
    "objectID": "slides/lec-8.html#introduction",
    "href": "slides/lec-8.html#introduction",
    "title": "Week 8: Regression Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nWe can use more than one predictor variable to explain the variation in the outcome variable\n\nAdd more terms to our regression equation to represent each predictor variable\n\nEach term has a coefficient that indicates how much the outcome variable changes when that predictor variable increases by one unit"
  },
  {
    "objectID": "slides/lec-8.html#example-parenthood-data-2",
    "href": "slides/lec-8.html#example-parenthood-data-2",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nOutcome variable: dani.grump (Dani’s grumpiness)\nPredictor variables: dani.sleep (Dani’s hours of sleep) and baby.sleep (Baby’s hours of sleep)\n\nRegression equation: \\(Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\)\n\\(Y_i\\): Dani’s grumpiness on day \\(i\\)\n\\(X_{i1}\\): Dani’s hours of sleep on day \\(i\\)\n\\(X_{i2}\\): Baby’s hours of sleep on day \\(i\\)\n\\(b_0\\): Intercept\n\\(b_1\\): Coefficient for Dani’s sleep\n\\(b_2\\): Coefficient for Baby’s sleep\n\\(\\epsilon_i\\): Error term on day \\(i\\)"
  },
  {
    "objectID": "slides/lec-8.html#estimating-the-coefficients-in-multiple-regression",
    "href": "slides/lec-8.html#estimating-the-coefficients-in-multiple-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Estimating the coefficients in multiple regression",
    "text": "Estimating the coefficients in multiple regression\n\nWe want to find the coefficients that minimize the sum of squared residuals\nResiduals are the differences between the observed and predicted values of the outcome variable\nWe use a similar method as in simple regression, but with more terms in the equation"
  },
  {
    "objectID": "slides/lec-8.html#doing-it-in-jamovi",
    "href": "slides/lec-8.html#doing-it-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Doing it in jamovi",
    "text": "Doing it in jamovi\n\n\n\n\n\njamovi can estimate multiple regression models easily\nWe just need to add more variables to the Covariates box in the analysis\njamovi will output the estimated coefficients and other statistics for each predictor variable\nThe Table shows the coefficients for dani.sleep and baby.sleep as predictors of dani.grump"
  },
  {
    "objectID": "slides/lec-8.html#interpreting-the-coefficients-in-multiple-regression",
    "href": "slides/lec-8.html#interpreting-the-coefficients-in-multiple-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Interpreting the coefficients in multiple regression",
    "text": "Interpreting the coefficients in multiple regression\n\nThe coefficients tell us how much the outcome variable changes when one predictor variable increases by one unit, holding the other predictor variables constant\nThe larger the absolute value of the coefficient, the stronger the effect of that predictor variable on the outcome variable\nThe sign of the coefficient indicates whether the effect is positive or negative"
  },
  {
    "objectID": "slides/lec-8.html#example-parenthood-data-3",
    "href": "slides/lec-8.html#example-parenthood-data-3",
    "title": "Week 8: Regression Analysis",
    "section": "Example: Parenthood data",
    "text": "Example: Parenthood data\n\nCoefficient (slope) for dani.sleep: -8.94\n\nInterpretation: Each additional hour of sleep reduces Dani’s grumpiness by 8.94 points, regardless of how much sleep the baby gets\n\nCoefficient (slope) for baby.sleep: 0.01\n\nInterpretation: Each additional hour of sleep for the baby increases Dani’s grumpiness by 0.01 points, regardless of how much sleep Dani gets"
  },
  {
    "objectID": "slides/lec-8.html#quantifying-the-fit-of-the-regression-model",
    "href": "slides/lec-8.html#quantifying-the-fit-of-the-regression-model",
    "title": "Week 8: Regression Analysis",
    "section": "Quantifying the fit of the regression model",
    "text": "Quantifying the fit of the regression model\n\nWe want to know how well our regression model predicts the outcome variable\nWe can compare the predicted values ( \\(\\hat{Y}_i\\) ) to the observed values ( \\(Y_i\\) ) using two sums of squares\n\nResidual sum of squares ( \\(SS_{res}\\) ): measures how much error there is in our predictions\nTotal sum of squares ( \\(SS_{tot}\\) ): measures how much variability there is in the outcome variable"
  },
  {
    "objectID": "slides/lec-8.html#the-r2-value-effect-size",
    "href": "slides/lec-8.html#the-r2-value-effect-size",
    "title": "Week 8: Regression Analysis",
    "section": "The \\(R^2\\) value (effect size)",
    "text": "The \\(R^2\\) value (effect size)\n\nThe \\(R^2\\) value is a proportion that tells us how much of the variability in the outcome variable is explained by our regression model\nIt is calculated as:\n\n\\[R^2=1-\\frac{SS_{res}}{SS_{tot}}\\]\n\nIt ranges from 0 to 1, with higher values indicating better fit\nIt can be interpreted as the percentage of variance explained by our regression model"
  },
  {
    "objectID": "slides/lec-8.html#the-relationship-between-regression-and-correlation",
    "href": "slides/lec-8.html#the-relationship-between-regression-and-correlation",
    "title": "Week 8: Regression Analysis",
    "section": "The relationship between regression and correlation",
    "text": "The relationship between regression and correlation\n\nRegression and correlation are both ways of measuring the strength and direction of a linear relationship between two variables\nFor a simple regression model with one predictor variable, the \\(R^2\\) value is equal to the square of the Pearson correlation coefficient (\\(r^2\\))\n\nRunning a Pearson correlation is equivalent to running a simple linear regression model"
  },
  {
    "objectID": "slides/lec-8.html#the-adjusted-r2-value",
    "href": "slides/lec-8.html#the-adjusted-r2-value",
    "title": "Week 8: Regression Analysis",
    "section": "The adjusted \\(R^2\\) value",
    "text": "The adjusted \\(R^2\\) value\n\nThe adjusted \\(R^2\\) value is a modified version of the \\(R^2\\) value that takes into account the number of predictors in the model\n\nThe adjusted \\(R^2\\) value adjusts for the degrees of freedom in the model\n\nIt increases only if adding a predictor improves the model more than expected by chance"
  },
  {
    "objectID": "slides/lec-8.html#which-one-to-report-r2-or-adjusted-r2",
    "href": "slides/lec-8.html#which-one-to-report-r2-or-adjusted-r2",
    "title": "Week 8: Regression Analysis",
    "section": "Which one to report: \\(R^2\\) or adjusted \\(R^2\\)?",
    "text": "Which one to report: \\(R^2\\) or adjusted \\(R^2\\)?\n\nThere is no definitive answer to this question\nIt depends on your preference and your research question\nSome factors to consider are:\n\nInterpretability: \\(R^2\\) is easier to understand and explain\nBias correction: Adjusted \\(R^2\\) is less likely to overestimate the model performance\nHypothesis testing: There are other ways to test if adding a predictor improves the model significantly"
  },
  {
    "objectID": "slides/lec-8.html#hypothesis-tests-for-regression-models",
    "href": "slides/lec-8.html#hypothesis-tests-for-regression-models",
    "title": "Week 8: Regression Analysis",
    "section": "Hypothesis tests for regression models",
    "text": "Hypothesis tests for regression models\n\nWe can use hypothesis tests to evaluate the significance of our regression model and its coefficients\nThere are two types of hypothesis tests for regression models:\n\nTesting the model as a whole: Is there any relationship between the predictors and the outcome?\nTesting a specific coefficient: Is a particular predictor significantly related to the outcome?"
  },
  {
    "objectID": "slides/lec-8.html#test-the-model-as-a-whole",
    "href": "slides/lec-8.html#test-the-model-as-a-whole",
    "title": "Week 8: Regression Analysis",
    "section": "Test the model as a whole",
    "text": "Test the model as a whole\n\\(H_0\\): there is no relationship between the predictors and the outcome\n\\(H_a\\): data follow the regression model\n\\[F=\\frac{(R^2/K)}{(1-R^2)/(N-K-1)}\\]\n\nwhere \\(R^2\\) is the proportion of variance explained by our model, \\(K\\) is the number of predictors, and \\(N\\) is the number of observations\nThe F-test statistic follows an F-distribution with \\(K\\) and \\(N-K-1\\) degrees of freedom\nWe can use a p-value to determine if our F-test statistic is significant\n jamovi can do this for us!"
  },
  {
    "objectID": "slides/lec-8.html#tests-for-individual-coefficients",
    "href": "slides/lec-8.html#tests-for-individual-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "Tests for Individual Coefficients",
    "text": "Tests for Individual Coefficients\n\nThe F-test checks if the model as a whole is performing better than chance\nIf the F-test is not significant, then the regression model may not be good\nHowever, passing the F-test does not imply that the model is good"
  },
  {
    "objectID": "slides/lec-8.html#example-of-multiple-linear-regression",
    "href": "slides/lec-8.html#example-of-multiple-linear-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Example of Multiple Linear Regression",
    "text": "Example of Multiple Linear Regression\n\nIn a multiple linear regression model with baby.sleep and dani.sleep as predictors:\n\nThe estimated regression coefficient for baby.sleep is small (0.01) compared to dani.sleep (-.8.95)\nThis suggests that only dani.sleep matters in predicting grumpiness"
  },
  {
    "objectID": "slides/lec-8.html#hypothesis-testing-for-regression-coefficients",
    "href": "slides/lec-8.html#hypothesis-testing-for-regression-coefficients",
    "title": "Week 8: Regression Analysis",
    "section": "Hypothesis Testing for Regression Coefficients",
    "text": "Hypothesis Testing for Regression Coefficients\n\nA t-test can be used to test if a regression coefficient is significantly different from zero\n\n\\(H_0\\): b = 0 (the true regression coefficient is zero)\n\\(H_0\\): b ≠ 0 (the true regression coefficient is not zero)"
  },
  {
    "objectID": "slides/lec-8.html#running-hypothesis-tests-in-jamovi",
    "href": "slides/lec-8.html#running-hypothesis-tests-in-jamovi",
    "title": "Week 8: Regression Analysis",
    "section": "Running Hypothesis Tests in Jamovi",
    "text": "Running Hypothesis Tests in Jamovi\n\nTo compute statistics, check relevant options and run regression in jamovi\nSee result in the next slide"
  },
  {
    "objectID": "slides/lec-8.html#output",
    "href": "slides/lec-8.html#output",
    "title": "Week 8: Regression Analysis",
    "section": "Output",
    "text": "Output\n\n\nModel Coefficients\n\nLocated at bottom of jamovi analysis results\nEach row refers to one coefficient in regression model\nFirst row is intercept term; later rows look at each predictor\n\nCoefficient Information\n\nFirst column: estimate of b\nSecond column: standard error estimate\nThird and fourth columns: lower and upper values for 95% confidence interval around b estimate\nFifth column: t-statistic ( \\(t = b / se(b)\\) )\nLast column: p-value for each test\n\nDegrees of Freedom\n\nNot listed in coefficients table itself\nAlways N - K - 1\nListed in table at top of output"
  },
  {
    "objectID": "slides/lec-8.html#interpretation",
    "href": "slides/lec-8.html#interpretation",
    "title": "Week 8: Regression Analysis",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\nConclusion\n\nThe current regression model may not be the best fit for the data\nDropping baby.sleep predictor entirely may improve the model\n\n\n\nThe model performs significantly better than chance\n\n\\(F(2,97) = 215.24\\), \\(p&lt; .001\\)\n\\(R^2 = .81\\) value indicates that the regression model accounts for 81% of the variability in the outcome measure\n\nIndividual Coefficients\n\nbaby.sleep variable has no significant effect\nAll work in this model is being done by the dani.sleep variable"
  },
  {
    "objectID": "slides/lec-8.html#assumptions-of-regression",
    "href": "slides/lec-8.html#assumptions-of-regression",
    "title": "Week 8: Regression Analysis",
    "section": "Assumptions of Regression",
    "text": "Assumptions of Regression\nThe linear regression model relies on several assumptions.\n\nLinearity: The relationship between X and Y is assumed to be linear.\nIndependence: Residuals are assumed to be independent of each other.\nNormality: The residuals are assumed to be normally distributed.\nEquality of Variance: The standard deviation of the residual is assumed to be the same for all values of Y-hat."
  },
  {
    "objectID": "slides/lec-8.html#assumptions-of-regression-cont.",
    "href": "slides/lec-8.html#assumptions-of-regression-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Assumptions of Regression, cont.",
    "text": "Assumptions of Regression, cont.\nAlso…\n\nUncorrelated Predictors: In a multiple regression model, predictors should not be too strongly correlated with each other.\n\nStrongly correlated predictors (collinearity) can cause problems when evaluating the model.\n\nNo “Bad” Outliers: The regression model should not be too strongly influenced by one or two anomalous data points.\n\nAnomalous data points can raise questions about the adequacy of the model and trustworthiness of data."
  },
  {
    "objectID": "slides/lec-8.html#checking-for-linearity",
    "href": "slides/lec-8.html#checking-for-linearity",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for linearity",
    "text": "Checking for linearity\n\n\n\n\nChecking Linearity\n\nIt is important to check for the linearity of relationships between predictors and outcomes.\n\nPlotting Relationships\n\nOne way to check for linearity is to plot the relationship between predicted values and observed values for the outcome variable.\n\nUsing Jamovi\n\nIn Jamovi, you can save predicted values to the dataset and then draw a scatterplot of observed against predicted (fitted) values.\n\nInterpreting Results\n\nIf the plot looks approximately linear, then it suggests that your model is not doing too badly. However, if there are big departures from linearity, it suggests that changes need to be made."
  },
  {
    "objectID": "slides/lec-8.html#checking-for-linearity-cont.",
    "href": "slides/lec-8.html#checking-for-linearity-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for linearity, cont.",
    "text": "Checking for linearity, cont.\n\n\n\n\nTo get a more detailed picture of linearity, it can be helpful to look at the relationship between predicted values and residuals.\nUsing Jamovi\n\nIn Jamovi, you can save residuals to the dataset and then draw a scatterplot of predicted values against residual values.\n\nInterpreting Results\n\nIdeally, the relationship between predicted values and residuals should be a straight, perfectly horizontal line. In practice, we’re looking for a reasonably straight or flat line. This is a matter of judgement."
  },
  {
    "objectID": "slides/lec-8.html#checking-for-normality-residuals",
    "href": "slides/lec-8.html#checking-for-normality-residuals",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for normality (residuals)",
    "text": "Checking for normality (residuals)\n\n\n\n\nRegression models rely on a normality assumption: the residuals should be normally distributed.\nUsing Jamovi\n\nIn Jamovi, you can draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option.\n\nInterpreting Results\n\nThe output shows the standardized residuals plotted as a function of their theoretical quantiles according to the regression model. The dots should be somewhat near the line."
  },
  {
    "objectID": "slides/lec-8.html#checking-for-normality-residuals-cont.",
    "href": "slides/lec-8.html#checking-for-normality-residuals-cont.",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for normality (residuals), cont.",
    "text": "Checking for normality (residuals), cont.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChecking Relationship between Predicted Values and Residuals\n\nIn Jamovi, you can use the ‘Residuals Plots’ option to check the relationship between predicted values and residuals.\nThe output provides a scatterplot for each predictor variable, the outcome variable, and the predicted values against residuals.\n\nInterpreting Results\n\nWe are looking for a fairly uniform distribution of dots with no clear bunching or patterning.\n\nThe dots are fairly evenly spread across the whole plot \n\nIssues with the relationship between predicted values and residuals? \n\nTransform one or more of the variables (Box-Cox Transform in jamovi)"
  },
  {
    "objectID": "slides/lec-8.html#checking-for-equality-of-variance",
    "href": "slides/lec-8.html#checking-for-equality-of-variance",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for equality of variance",
    "text": "Checking for equality of variance\n\n\n\n\nRegression models make an assumption of equality (homogeneity) of variance.\n\nThis means that the variance of the residuals is assumed to be constant.\n\nPlotting Equality of Variance in Jamovi\n\nTo check this assumption in Jamovi, first calculate the square root of the absolute size of the residual.\n\nCompute this new variable using the formula SQRT(ABS(Residuals))\n\nThen plot this against the predicted values.\nThe plot should show a straight horizontal line running through the middle."
  },
  {
    "objectID": "slides/lec-8.html#checking-for-collineary",
    "href": "slides/lec-8.html#checking-for-collineary",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for Collineary",
    "text": "Checking for Collineary\n\n\n\n\n\nVariance Inflation Factors (VIFs) can be used to determine if predictors in a regression model are too highly correlated with each other.\n\nEach predictor has an associated VIF.\n\nIn Jamovi, click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options to see VIF values.\nInterpreting VIF\n\nA VIF of 1 means no correlation among the predictor and the remaining predictor variables\nVIFs exceeding 4 warrant further investigation\nVIFs exceeding 10 are signs of serious multicollinearity requiring correction"
  },
  {
    "objectID": "slides/lec-8.html#checking-for-outliers",
    "href": "slides/lec-8.html#checking-for-outliers",
    "title": "Week 8: Regression Analysis",
    "section": "Checking for outliers",
    "text": "Checking for outliers\n\n\n\n\n\nUsed in regression analysis to identify influential data points that may negatively affect your regression model\nDatasets with a large number of highly influential points might not be suitable for linear regression without further processing such as outlier removal or imputation\nIdentifying Outliers\n\nA general rule of thumb: Cook’s distance greater than 1 is often considered large\n\nWhat if the value is greater than 1?\n\nremove the outlier and run the regression again\nHow? In jamovi you can save the Cook’s distance values to the dataset, then draw a boxplot of the Cook’s distance values to identify the specific outliers."
  }
]